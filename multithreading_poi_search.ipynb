{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import unary_union\n",
    "import itertools\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import requests\n",
    "from shapely.ops import unary_union\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_api_keys(file_path):\n",
    "    \"\"\"\n",
    "    Load API keys from a file into a list.\n",
    "    :param file_path: Path to the API keys file.\n",
    "    :return: A list of API keys.\n",
    "    \"\"\"\n",
    "    api_keys = []\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                # Split the line at the '#' character and take the part before it\n",
    "                api_key = line.split('#')[0].strip()\n",
    "                if api_key:  # Avoid adding empty lines\n",
    "                    api_keys.append(api_key)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: API keys file '{file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading API keys: {e}\")\n",
    "    return api_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your API keys file\n",
    "api_keys_file = r\"C:\\Users\\Hewan Shrestha\\Desktop\\google_poi_search\\data\\api_keys.txt\"\n",
    "\n",
    "# Load the API keys\n",
    "api_keys = load_api_keys(api_keys_file)\n",
    "\n",
    "# Create an iterator to cycle through the API keys\n",
    "api_key_iterator = itertools.cycle(api_keys)\n",
    "\n",
    "# Dictionary to track API request counts\n",
    "api_request_counts = {key: 0 for key in api_keys}\n",
    "\n",
    "# Lock for thread-safe operations\n",
    "api_lock = threading.Lock()\n",
    "\n",
    "# Function to increment API request count\n",
    "def increment_api_request_count(api_key):\n",
    "    with api_lock:\n",
    "        api_request_counts[api_key] += 1\n",
    "\n",
    "# Function to get the next API key\n",
    "def get_next_api_key():\n",
    "    with api_lock:\n",
    "        api_key = next(api_key_iterator)\n",
    "        print(f\"************************* \\nAPI Key used: {api_key} \\n*************************\\n\")  # Separator with API key used\n",
    "        return api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"C:\\Users\\Hewan Shrestha\\Desktop\\detect-car-in-LR-satellite-images\\Google_Places\\new_data_collection_google_poi_to_outscraper\\working_folder\\csvs\\saarbrucken_new\"\n",
    "# File path to save POIs\n",
    "output_file = os.path.join(output_dir, \"multithreading_saarbrucken_results.csv\")\n",
    "\n",
    "# Create the CSV file and write the header\n",
    "if not os.path.exists(output_file):\n",
    "    with open(output_file, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"place_id\", \"Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save POIs to a CSV file\n",
    "def save_pois_to_csv(pois, poi_type):\n",
    "    with api_lock:  # Ensure thread-safe writes\n",
    "        with open(output_file, \"a\", newline='', encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for poi_id, poi_type in pois:\n",
    "                writer.writerow([poi_id, poi_type])\n",
    "        print(f\"Saved {len(pois)} POIs for type '{poi_type}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_file = os.path.join(output_dir, \"api_usage.json\")\n",
    "# Function to save API usage data to a file\n",
    "def save_api_usage_to_file(output_json_file):\n",
    "    with api_lock:\n",
    "        with open(output_json_file, \"w\") as f:\n",
    "            json.dump(api_request_counts, f, indent=4)\n",
    "    print(f\"API usage data saved to {output_json_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_proxies(file_path, username, password):\n",
    "    proxies = []\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                proxy = line.strip()\n",
    "                if proxy:\n",
    "                    # Format proxy with username and password\n",
    "                    formatted_proxy = f\"http://{username}:{password}@{proxy}\"\n",
    "                    proxies.append(formatted_proxy)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Proxy file '{file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading proxies: {e}\")\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your proxies file\n",
    "proxy_file = r\"C:\\Users\\Hewan Shrestha\\Desktop\\google_poi_search\\data\\proxies.txt\"\n",
    "\n",
    "# Define the username and password for proxies\n",
    "username = \"iweber02\"\n",
    "password = \"qp9dQbDM\"\n",
    "\n",
    "# Load proxies with authentication\n",
    "proxy_list = load_proxies(proxy_file, username, password)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_pois(rectangle, api_key, poi_type, data, retries=5, delay=1):\n",
    "    print(f\"\\nSearching POIs for type '{poi_type}' using API key '{api_key}'.\\nBounding box: {rectangle.bounds}\")\n",
    "    min_lng, min_lat, max_lng, max_lat = rectangle.bounds\n",
    "    \n",
    "    # Add locationRestriction to the data payload\n",
    "    data[\"locationRestriction\"] = {\n",
    "        \"rectangle\": {\n",
    "            \"low\": {\"latitude\": min_lat, \"longitude\": min_lng},\n",
    "            \"high\": {\"latitude\": max_lat, \"longitude\": max_lng}\n",
    "        }\n",
    "    }\n",
    "    url = \"https://places.googleapis.com/v1/places:searchText\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"X-Goog-Api-Key\": api_key,\n",
    "        \"X-Goog-FieldMask\": \"places.id\"\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Use the proxy list (optional)\n",
    "            proxies = {'https': proxy for proxy in proxy_list}\n",
    "            print(f\"Attempt {attempt+1}, using proxies: {proxies}\")\n",
    "            \n",
    "            # Make the API request\n",
    "            response = requests.post(url, headers=headers, json=data, proxies=proxies)\n",
    "            \n",
    "            # Increment API request count\n",
    "            increment_api_request_count(api_key)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Parse the response for POIs\n",
    "                for place in response.json().get('places', []):\n",
    "                    place_id = place['id']\n",
    "                    results.append((place_id, poi_type))\n",
    "                return results  # Return results if the request is successful\n",
    "            \n",
    "            elif response.status_code == 429:  # Rate limit error\n",
    "                print(f\"Rate limit hit for API key: {api_key}. Switching API key...\")\n",
    "                # Switch to the next API key\n",
    "                api_key = get_next_api_key()\n",
    "            \n",
    "            else:\n",
    "                print(f\"Error occurred: {response.content}\")\n",
    "                break  # Break if the error is not recoverable\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during request: {e}\")\n",
    "        \n",
    "        # Apply exponential backoff\n",
    "        time.sleep(delay)\n",
    "        delay *= 2  # Double the delay for the next attempt\n",
    "    \n",
    "    print(f\"Failed to retrieve POIs for type '{poi_type}' after {retries} attempts.\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_box(boundary_box, depth, quadrant_number, use_overlap, overlap=0.0006):\n",
    "    min_lng, min_lat, max_lng, max_lat = boundary_box.bounds\n",
    "    mid_lng, mid_lat = (min_lng + max_lng) / 2, (min_lat + max_lat) / 2\n",
    "\n",
    "    if use_overlap:\n",
    "        # Create quadrants with overlap\n",
    "        quadrants = [\n",
    "            box(min_lng, min_lat, mid_lng + overlap, mid_lat + overlap),  # Bottom-left\n",
    "            box(mid_lng - overlap, min_lat, max_lng, mid_lat + overlap),  # Bottom-right\n",
    "            box(min_lng, mid_lat - overlap, mid_lng + overlap, max_lat),  # Top-left\n",
    "            box(mid_lng - overlap, mid_lat - overlap, max_lng, max_lat)   # Top-right\n",
    "        ]\n",
    "    else:\n",
    "        # Create quadrants without overlap\n",
    "        quadrants = [\n",
    "            box(min_lng, min_lat, mid_lng, mid_lat),  # Bottom-left\n",
    "            box(mid_lng, min_lat, max_lng, mid_lat),  # Bottom-right\n",
    "            box(min_lng, mid_lat, mid_lng, max_lat),  # Top-left\n",
    "            box(mid_lng, mid_lat, max_lng, max_lat)   # Top-right\n",
    "        ]\n",
    "    \n",
    "    # # Save each quadrant as a GeoJSON file with depth, quadrant number, and boundary coordinates\n",
    "    # for i, quadrant in enumerate(quadrants):\n",
    "    #     gdf = gpd.GeoDataFrame(geometry=[quadrant], crs=\"EPSG:4326\")\n",
    "    #     filename = rf\"C:\\Users\\Hewan Shrestha\\Desktop\\detect-car-in-LR-satellite-images\\Google_Places\\new_data_collection_google_poi_to_outscraper\\working_folder\\csvs\\dudweiler_test\\geojsons\\depth_{depth}_quadrant_{quadrant_number}_{min_lng}_{min_lat}_{max_lng}_{max_lat}_{i + 1}.geojson\"\n",
    "    #     gdf.to_file(filename, driver=\"GeoJSON\")\n",
    "    #     print(f\"Saved {filename} with depth {depth} and quadrant number {quadrant_number}, coordinates: ({min_lng}, {min_lat}), ({max_lng}, {max_lat})\")\n",
    "        \n",
    "    return quadrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the longest side of a bounding box using geodesic distance\n",
    "def get_longest_side(bounding_box):\n",
    "    min_lng, min_lat, max_lng, max_lat = bounding_box.bounds\n",
    "    \n",
    "    # Define the four corners of the bounding box\n",
    "    bottom_left = (min_lat, min_lng)\n",
    "    bottom_right = (min_lat, max_lng)\n",
    "    top_left = (max_lat, min_lng)\n",
    "    top_right = (max_lat, max_lng)\n",
    "\n",
    "    # Calculate the distances between the four corners\n",
    "    width = int(geodesic(bottom_left, bottom_right).meters)  # Distance along longitude (width)\n",
    "    height = int(geodesic(bottom_left, top_left).meters)    # Distance along latitude (height)\n",
    "\n",
    "    # Find the longest side\n",
    "    longest_side = max(width, height)\n",
    "\n",
    "    # Print the longest side length for debugging\n",
    "    print(f\"Longest side length (in meters): {longest_side:.2f}\")\n",
    "\n",
    "    # Return the longest side as an integer\n",
    "    return int(longest_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to recursively search for POIs using a quadtree approach\n",
    "def quadtree_search(bounding_box, api_key, poi_type, data, threshold, quadrant_number=1, searched_areas=None, overlap=0.0007, depth=0):\n",
    "    if searched_areas is None:\n",
    "        searched_areas = set()\n",
    "\n",
    "    results = set()\n",
    "    use_overlap = True\n",
    "    \n",
    "    # Normalize bounding box coordinates to prevent precision errors\n",
    "    def normalize_coords(coords, precision=8):\n",
    "        return tuple(round(c, precision) for c in coords)\n",
    "\n",
    "    bounding_box_coords = normalize_coords(bounding_box.bounds)\n",
    "    if bounding_box_coords in searched_areas:\n",
    "        print(f\"Bounding box {bounding_box_coords} has already been searched. Skipping...\")\n",
    "        return results  # Skip this bounding box but continue searching others\n",
    "\n",
    "    # Get the longest side using geodesic library\n",
    "    longest_side = get_longest_side(bounding_box)\n",
    "\n",
    "    # Check for invalid bounding box\n",
    "    if longest_side == 0:\n",
    "        print(f\"Bounding box {bounding_box_coords} is invalid (longest side = 0.0). Skipping...\")\n",
    "        return results\n",
    "\n",
    "    # Add bounding box to searched areas\n",
    "    searched_areas.add(bounding_box_coords)\n",
    "    print(f\"\\nSearching depth {depth}, quadrant {quadrant_number}, bounding box {bounding_box_coords}\")\n",
    "\n",
    "    pois = search_pois(bounding_box, api_key, poi_type, data)\n",
    "    print(f\"Number of places found: {len(pois)}\")\n",
    "\n",
    "    # Save POIs to the CSV file\n",
    "    save_pois_to_csv(pois, poi_type)\n",
    "\n",
    "    # Update the results set with found POIs\n",
    "    results.update(pois)\n",
    "\n",
    "    # If the longest side is less than 10 meters, do not divide further\n",
    "    if longest_side < 50 and (len(pois) > threshold or len(pois) < threshold):\n",
    "        print(f\"Longest side {longest_side} meters is less than 50. Adding results and not dividing further.\")\n",
    "        return results\n",
    "\n",
    "    # Determine whether to divide the box based on POIs found and side length\n",
    "    if len(pois) > threshold:\n",
    "        # If POIs are above the threshold, divide the bounding box\n",
    "        if longest_side < 200:\n",
    "            # If the longest side is smaller than 200 meters, do not use overlap\n",
    "            use_overlap = False\n",
    "            print(f\"Longest side {longest_side} meters is less than 200, dividing without overlap.\")\n",
    "        else:\n",
    "            # If the longest side is larger than 200 meters, use overlap\n",
    "            use_overlap = True\n",
    "            print(f\"Longest side {longest_side} meters is above 200, dividing with overlap.\")\n",
    "        \n",
    "        quadrants = divide_box(bounding_box, depth=depth, quadrant_number=quadrant_number, use_overlap=use_overlap, overlap=overlap)\n",
    "        print(\"*\" * 50)\n",
    "        print(\"\\n\" * 2)\n",
    "\n",
    "        # Process quadrants sequentially at the current depth\n",
    "        for i, quadrant in enumerate(quadrants):\n",
    "            print(f\"Searching depth {depth+1} at quadrant {i+1}\")\n",
    "            results.update(quadtree_search(quadrant, api_key, poi_type, data, threshold, i+1, searched_areas, overlap, depth+1))\n",
    "\n",
    "    else:\n",
    "        print(f\"Number of POIs {len(pois)} is below threshold {threshold}. Continuing search.\")\n",
    "    \n",
    "    # Once all quadrants at the current depth are processed, return the results for this level.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_poi_type(poi_type, rectangle, threshold):\n",
    "    print(f\"\\n************************* \\nStarting processing for POI type: {poi_type} \\n*************************\\n\")  # Separator for POI type start\n",
    "    data = {\"textQuery\": poi_type}\n",
    "    \n",
    "    # Get the next API key for the current thread\n",
    "    api_key = get_next_api_key()\n",
    "    \n",
    "    results = quadtree_search(rectangle, api_key, poi_type, data, threshold)\n",
    "    print(f\"\\n************************* \\nFinished processing for POI type: {poi_type}, found {len(results)} POIs \\n*************************\\n\")  # Separator for POI type end\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the GeoJSON file\n",
    "city_gdf = gpd.read_file(r\"C:\\Users\\Hewan Shrestha\\Desktop\\google_poi_search\\shapefiles\\saarbrucken.geojson\")\n",
    "city_polygon = unary_union(city_gdf['geometry'])\n",
    "\n",
    "# Create a bounding box for the city polygon\n",
    "minx, miny, maxx, maxy = city_polygon.bounds\n",
    "rectangle = box(minx, miny, maxx, maxy)\n",
    "\n",
    "# Open the text file in read mode to read POI types\n",
    "with open(r'C:\\Users\\Hewan Shrestha\\Desktop\\google_poi_search\\data\\outscraper_filtered_subtypes.txt', 'r') as file:\n",
    "    places_of_interest = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the profiler\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "# Code block for multithreading and processing POIs\n",
    "start_time_main = time.time()\n",
    "\n",
    "# Run the search for each POI type in parallel\n",
    "with ThreadPoolExecutor(max_workers=len(api_keys)) as executor:\n",
    "    future_to_poi = {executor.submit(process_poi_type, poi_type, rectangle, threshold): poi_type for poi_type in places_of_interest}\n",
    "    all_results = set()\n",
    "    \n",
    "    for future in future_to_poi:\n",
    "        try:\n",
    "            results = future.result()\n",
    "            all_results.update(results)\n",
    "            print(f\"\\n************************* \\nNumber of POIs found for {future_to_poi[future]}: {len(results)} \\n*************************\\n\")  # Separator for individual POI results\n",
    "            print(\"*\"*50)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing POI type: {future_to_poi[future]} - {e}\\n\")\n",
    "\n",
    "# Final results count\n",
    "print(f\"\\n************************* \\nTotal unique POIs after all searches: {len(all_results)} \\n*************************\\n\")  # Separator for final count\n",
    "\n",
    "\n",
    "end_time_main = time.time()\n",
    "print(f\"Total execution time for multithreading: {end_time_main - start_time_main:.2f} seconds\")\n",
    "\n",
    "# End profiling\n",
    "pr.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print profiling report\n",
    "s = StringIO()\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')\n",
    "ps.print_stats()\n",
    "\n",
    "# Save the profiling report to a file (optional)\n",
    "with open(r'C:\\Users\\Hewan Shrestha\\Desktop\\detect-car-in-LR-satellite-images\\Google_Places\\new_data_collection_google_poi_to_outscraper\\working_folder\\csvs\\saarbrucken_new\\profiling_report.txt', 'w') as f:\n",
    "    f.write(s.getvalue())\n",
    "\n",
    "print(s.getvalue())\n",
    "\n",
    "\n",
    "save_api_usage_to_file(output_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
